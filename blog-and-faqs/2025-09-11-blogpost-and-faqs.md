# EMNLP 2025 Demo Chairs Blogpost and FAQs

2025-09-11 by Ivan Habernal, Jörg Tiedemann, Peter Schulam (chairs)

## Reacting to: “The reviewers were incompetent”

Reviewers were recruited through open calls for reviewers (self-nomination), we contacted corresponding authors of previously published demo papers (ACL* conferences 2024-2025), and asked previous reviewers from EMNLP 2024. We made sure that self-nominated reviewers were qualified by checking their papers in the ACL Anthology on a case-by-case basis. Through this process, we invited 343 reviewers through OpenReview from which 185 eventually accepted the invite. This number includes all further emergency reviewers, who were approached by a call for emergency reviewers on various platforms, through personal contacts, and also through personal connections of Area Chairs.

Most reviewers delivered reviews on time (we reminded them a few times) but about 10% of reviewers simply disappeared and did not reply to any e-mail, either through OpenReview or personal. This is very unfortunate and might signal something about the state of reviewing in 2025; we have also heard some anecdotal evidence of a review burnout. In any case, single-blind peer review is the system our community has agreed on to use for the demo track and we believe the benefits (preferring quality over quantity) still overweight the negatives (being noisy, possibly having adversaries among the reviewers, low quality or LLM-generated reviews).

## Reacting to: “Our paper rejected despite positive reviews and/or positive meta-review”

We relied on experienced Area Chairs (ACs) to make final recommendations, and in the majority cases we followed them. ACs were selected from previous EMNLP area chairs and from professional/personal connections. ACs are experienced researchers who are diligent and (mostly) extremely reliable with great communication skills to handle PCs requests and emergencies in a timely manner and pay attention to detail. We recruited 31 area chairs, and are very thankful for their excellent work. ACs also knew “their” reviewers and could factor in their competency or the lack thereof. Despite all this, we cannot completely prevent any misunderstandings or mistakes in meta-reviews and/or recommendations.

There were cases of discrepancies between the reviewer scores and the meta-review. In 14 cases, the meta-review suggested rejection despite above-average scores. One PC then reviewed the reviews, meta-review, and the paper itself carefully and made the final decision, which was double-checked by a second PC. We as PCs did not provide a detailed review to the authors. The same procedure was done for another 24 papers in which discrepancies had to be resolved.

Many of the papers marked by our ACs as maybe-accept (some erroneously named as accepted as poster; also see below) had to be rejected in the end because of the constraints given by the EMNLP organization. Not all acceptable papers can be taken in and there were simply other papers with better reviews, and ACs clearly recommended accepting them. 

## Reacting to: “Our paper was rejected although the AC gave it ‘accept (poster)’”

This label is a bug by a default configuration in OpenReview (see https://github.com/habernal/emnlp2025-demos-openreview?tab=readme-ov-file#issues-with-values-regarding-recommendation-accept-reject-etc ) and this recommendation was not considered in any decision stage. The actual recommendation scale for ACs was “1 = reject, 2 = maybe accept, 3 = accept”.

## Reacting to: “One reviewer is a competitor of ours and rejected the paper from an adversarial position”

This one is a tough one and, honestly, there is a little we can do about it. See our comments on the reviewers above. This situation is possible, but the PCs are not able to prove or disprove it.

## Reacting to: “Our paper had only two reviews, should have been three at least”

Yes, ideally. There simply were not enough people willing to review for us (see the stats above). We believe in the future we should condition submissions on reviews, so everyone “gives and takes”, as in ARR now. Or, ideally, couple demos with ARR completely and not try to run it in parallel.

# FAQs

## Reacting to: “We cannot add an author”

Originally we disabled changing the authors, but in fact it is the sole responsibility to adhere to the authorship code of conduct:
https://www.aclweb.org/adminwiki/index.php/Authorship_Changes_Policy_for_ACL_Conference_Papers So we enabled changing the authors before the camera-ready deadline.

## Reacting to "How to address reviewer's comments"

Some author misunderstood what incorporating reviewers' comments to the final paper means. Basically, you have a chance to improve the paper (we allow on extra page, so 7 pages limit) and address some reviewer's comments directly in the paper. The reviews themselves will not be published.